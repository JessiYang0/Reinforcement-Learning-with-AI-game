{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lHPl6l9FqGX"
      },
      "source": [
        "## Designing game AI with Reinforcement learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Re-PV4pFqGb"
      },
      "source": [
        "## Introduction\n",
        "To design a neural network for simulating and playing the game Halite by Two Sigma using an Actor-Critic agent, let's outline the steps and components involved in creating this AI model. The Actor-Critic model, a reinforcement learning approach, consists of two main components:\n",
        "\n",
        "Actor: Determines the action to take based on the current state of the environment. In the context of Halite, this would involve deciding the direction each ship should move to collect halite or return it to a shipyard.\n",
        "\n",
        "Critic: Estimates the value function of the current state, or in other words, it evaluates how good the current state is for the agent. This helps in adjusting the policy defined by the actor towards more rewarding actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1hFkg4EFqGd"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "id": "A2tg-PH3-bfM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install kaggle-environments --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "70E_TWot-Y8A",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import sys\n",
        "import PIL.Image\n",
        "\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "\n",
        "from sklearn import preprocessing\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from kaggle_environments import evaluate, make\n",
        "from kaggle_environments.envs.halite.helpers import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9x4rOBK-Y8O",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "seed=123\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "# ensure repeatable random number generation.\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "#prevents TensorFlow from using multiple threads and makes the execution deterministic\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "# initializes the actual runtime environment used to run graphs and evaluate tensors\n",
        "tf.compat.v1.keras.backend.set_session(sess)\n",
        "#Setting the Keras backend to use this session.\n",
        "logging.disable(sys.maxsize)\n",
        "#This prevents any logs from being printed. \n",
        "global ship_\n",
        "#Defining a global variable called ship_ which will be accessible throughout the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i1LqBHWFqGf"
      },
      "source": [
        "## Analyzing the environment\n",
        "Lets take a tour of our environment and its settings first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "sV_2Kxnn-Y8c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "env = make(\"halite\", debug=True) #\"make\" function from kaggle_environments\n",
        "# initialize a Halite environment\n",
        "env.run([\"random\"]) # take random actions each turn\n",
        "env.render(mode=\"ipython\",width=800, height=600) #enders the environment visually after running the agen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bdT3Gpi-Y8o",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "env.configuration\n",
        "\n",
        "# {'episodeSteps': 400, The maximum number of steps per episode \n",
        "#  'agentExec': 'LOCAL',  execute locally, not remotely like on a cloud server\n",
        "#  'agentTimeout': 30, The timeout for the agent to return an action decision is 30 seconds\n",
        "#  'actTimeout': 6, The per-step action timeout is 6 seconds\n",
        "#  'runTimeout': 9600,  The maximum timeout for the full environment run \n",
        "#  'startingHalite': 24000, Each agent starts with 24000 initial Halite at the episode start\n",
        "#  'size': 21, The size of the game board is 21x21\n",
        "#  'spawnCost': 500, The cost to spawn a ship is 500\n",
        "#  'convertCost': 500, The cost to convert a ship is 500\n",
        "#  'moveCost': 0, The Halite cost for a ship to move is 0.\n",
        "#  'collectRate': 0.25, A ship collects 0.25 Halite per step  from a cell\n",
        "#  'regenRate': 0.02, Each tile regenerates 0.02 Halite per step, Each 'tile' can contain some amount of Halite resource\n",
        "#  'maxCellHalite': 500}  The maximum amount of Halite a tile can hold is 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89pvWtr2-Y8w",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "env.specification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkZuJWY4-Y85",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "env.specification.reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXP9fHLl-Y9D",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "env.specification.action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YE4r_pI-Y9J",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "env.specification.observation\n",
        "#'Serialized list of available halite per cell on the board"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McsAQVQ7FqGg"
      },
      "source": [
        "## The game begins\n",
        "So lets train our model with respect to random actions and see what happens..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZRli1_T-Y9Q",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def getDirTo(fromPos, toPos, size): # Returns the direction to move from one position to another\n",
        "    # input ship and target positions, size of the board\n",
        "    fromX, fromY = divmod(fromPos[0],size), divmod(fromPos[1],size)\n",
        "    toX, toY = divmod(toPos[0],size), divmod(toPos[1],size)\n",
        "    if fromY < toY: return ShipAction.NORTH \n",
        "    if fromY > toY: return ShipAction.SOUTH\n",
        "    if fromX < toX: return ShipAction.EAST\n",
        "    if fromX > toX: return ShipAction.WEST\n",
        "\n",
        "# Directions a ship can move\n",
        "directions = [ShipAction.NORTH, ShipAction.EAST, ShipAction.SOUTH, ShipAction.WEST]\n",
        "\n",
        "# Will keep track of whether a ship is collecting halite or carrying cargo to a shipyard\n",
        "ship_states = {}\n",
        "\n",
        "# Returns the commands we send to our ships and shipyards\n",
        "def simple_agent(obs, config): \n",
        "    # input Serialized list of available halite per cell on the board, and env config\n",
        "    size = config.size\n",
        "    board = Board(obs, config)\n",
        "    me = board.current_player\n",
        "    # If there are no ships, use first shipyard to spawn a ship.\n",
        "    if len(me.ships) == 0 and len(me.shipyards) > 0:\n",
        "        me.shipyards[0].next_action = ShipyardAction.SPAWN\n",
        "\n",
        "    # If there are no shipyards, convert first ship into shipyard.\n",
        "    if len(me.shipyards) == 0 and len(me.ships) > 0:\n",
        "        me.ships[0].next_action = ShipAction.CONVERT\n",
        "\n",
        "    for ship in me.ships:\n",
        "        if ship.next_action == None:\n",
        "\n",
        "            ### Part 1: Set the ship's state\n",
        "            if ship.halite < 200: # If cargo is too low, collect halite\n",
        "                ship_states[ship.id] = \"COLLECT\"\n",
        "            if ship.halite > 500: # If cargo gets very big, deposit halite\n",
        "                ship_states[ship.id] = \"DEPOSIT\"\n",
        "\n",
        "            ### Part 2: Use the ship's state to select an action\n",
        "            if ship_states[ship.id] == \"COLLECT\":\n",
        "                # If halite at current location running low,\n",
        "                # move to the adjacent square containing the most halite\n",
        "                if ship.cell.halite < 100:\n",
        "                    neighbors = [ship.cell.north.halite, ship.cell.east.halite,\n",
        "                                 ship.cell.south.halite, ship.cell.west.halite]\n",
        "                    best = max(range(len(neighbors)), key=neighbors.__getitem__)\n",
        "                    ship.next_action = directions[best]\n",
        "            if ship_states[ship.id] == \"DEPOSIT\":\n",
        "                # Move towards shipyard to deposit cargo\n",
        "                direction = getDirTo(ship.position, me.shipyards[0].position, size)\n",
        "                if direction: ship.next_action = direction\n",
        "\n",
        "    return me.next_actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "id": "uJMYO9VZ-Y9Y",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer = env.train([None, \"random\"]) #random agent #interact with env\n",
        "observation = trainer.reset() #reset environment #each cell halite reset\n",
        "while not env.done:\n",
        "    my_action = simple_agent(observation, env.configuration)\n",
        "    print(\"My Action\", my_action)\n",
        "    observation = trainer.step(my_action)[0]  #update each cell halite\n",
        "    print(\"Reward gained\",observation.players[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQeSpK_s-Y9g",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "env.render(mode=\"ipython\",width=800, height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK9-83waFqGh"
      },
      "source": [
        "## The Actor-Critic model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCPI5mpD-Y-F",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def ActorModel(num_actions,in_): # input number of actions, number of halite in each cell\n",
        "    common = tf.keras.layers.Dense(128, activation='tanh')(in_) #output 128 nodes after tanh activation\n",
        "    common = tf.keras.layers.Dense(32, activation='tanh')(common)\n",
        "    common = tf.keras.layers.Dense(num_actions, activation='softmax')(common) #each probability of action (5 action) \n",
        "\n",
        "    return common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAax9a8q-Y-L",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def CriticModel(in_): #number of halite in each cell\n",
        "    common = tf.keras.layers.Dense(128)(in_) #output 128 nodes after tanh activation\n",
        "    common = tf.keras.layers.ReLU()(common)\n",
        "    common = tf.keras.layers.Dense(32)(common)\n",
        "    common = tf.keras.layers.ReLU()(common)\n",
        "    common = tf.keras.layers.Dense(1)(common) #output value\n",
        "\n",
        "    return common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODputeIs-Y-T",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "input_ = tf.keras.layers.Input(shape=[441,]) #441 cell map size: 21*21\n",
        "model = tf.keras.Model(inputs=input_, outputs=[ActorModel(5,input_),CriticModel(input_)])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8a9NRpJ-Y-Z",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(lr=7e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_r9VCHQ-Y-e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "huber_loss = tf.keras.losses.Huber()\n",
        "action_probs_history = []\n",
        "critic_value_history = []\n",
        "rewards_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "num_actions = 5\n",
        "eps = np.finfo(np.float32).eps.item() #prevent Denominator to 0\n",
        "gamma = 0.99  # Discount factor for rewards\n",
        "env = make(\"halite\", debug=True)\n",
        "trainer = env.train([None,\"random\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7gtUD_BFqGh"
      },
      "source": [
        "## Encoding our moves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOB7Y9Di-Y-i",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "label_encoded = le.fit_transform(['NORTH', 'SOUTH', 'EAST', 'WEST', 'CONVERT'])\n",
        "label_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hspjmUQb-Y-o",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def getDirTo(fromPos, toPos, size):\n",
        "    fromX, fromY = divmod(fromPos[0],size), divmod(fromPos[1],size)\n",
        "    toX, toY = divmod(toPos[0],size), divmod(toPos[1],size)\n",
        "    if fromY < toY: return ShipAction.NORTH\n",
        "    if fromY > toY: return ShipAction.SOUTH\n",
        "    if fromX < toX: return ShipAction.EAST\n",
        "    if fromX > toX: return ShipAction.WEST\n",
        "\n",
        "# Directions a ship can move\n",
        "directions = [ShipAction.NORTH, ShipAction.EAST, ShipAction.SOUTH, ShipAction.WEST]\n",
        "\n",
        "def decodeDir(act_):\n",
        "    if act_ == 'NORTH':return directions[0]\n",
        "    if act_ == 'EAST':return directions[1]\n",
        "    if act_ == 'SOUTH':return directions[2]\n",
        "    if act_ == 'WEST':return directions[3]\n",
        "\n",
        "# Will keep track of whether a ship is collecting halite or carrying cargo to a shipyard\n",
        "ship_states = {}\n",
        "ship_ = 0\n",
        "def update_L1():\n",
        "    ship_+=1\n",
        "# Returns the commands we send to our ships and shipyards\n",
        "def advanced_agent(obs, config, action): #input Serialized list of available halite per cell on the board, and env config\n",
        "    size = config.size\n",
        "    board = Board(obs, config)\n",
        "    me = board.current_player\n",
        "    act = le.inverse_transform([action])[0]\n",
        "    global ship_\n",
        "\n",
        "   # If there are no ships, use first shipyard to spawn a ship.\n",
        "    if len(me.ships) == 0 and len(me.shipyards) > 0:\n",
        "        me.shipyards[ship_-1].next_action = ShipyardAction.SPAWN\n",
        "\n",
        "    # If there are no shipyards, convert first ship into shipyard.\n",
        "    if len(me.shipyards) == 0 and len(me.ships) > 0 and ship_==0:\n",
        "        me.ships[0].next_action = ShipAction.CONVERT\n",
        "    try:\n",
        "        if act=='CONVERT':\n",
        "            me.ships[0].next_action = ShipAction.CONVERT\n",
        "            update_L1()\n",
        "            if len(me.ships)==0 and len(me.shipyards) > 0:\n",
        "                me.shipyards[ship_-1].next_action = ShipyardAction.SPAWN\n",
        "        if me.ships[0].halite < 200:\n",
        "            ship_states[me.ships[0].id] = 'COLLECT'\n",
        "        if me.ships[0].halite > 800:\n",
        "            ship_states[me.ships[0].id] = 'DEPOSIT'\n",
        "\n",
        "        if ship_states[me.ships[0].id] == 'COLLECT':\n",
        "            if me.ships[0].cell.halite < 100:\n",
        "                me.ships[0].next_action = decodeDir(act)\n",
        "        if ship_states[me.ships[0].id] == 'DEPOSIT':\n",
        "            # Move towards shipyard to deposit cargo\n",
        "            direction = getDirTo(me.ships[0].position, me.shipyards[ship_-1].position, size)\n",
        "            if direction: me.ships[0].next_action = direction\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return me.next_actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYPUuqwa-Y-s",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "while not env.done:\n",
        "    state = trainer.reset() # Reset the environment (reset each cell halite, not Q function)\n",
        "    episode_reward = 0\n",
        "    with tf.GradientTape() as tape: # Record operations for automatic differentiation\n",
        "        for timestep in range(1,env.configuration.episodeSteps+200):\n",
        "            # of the agent in a pop up window. #each halite convert to tensor\n",
        "            state_ = tf.convert_to_tensor(state.halite)\n",
        "            state_ = tf.expand_dims(state_, 0)\n",
        "            # Predict action probabilities and estimated future rewards\n",
        "            # from environment state, cell map size: 21*21, each cell halite\n",
        "            action_probs, critic_value = model(state_) \n",
        "            critic_value_history.append(critic_value[0, 0])\n",
        "\n",
        "            # Sample action from action probability distribution\n",
        "            action = np.random.choice(num_actions, p=np.squeeze(action_probs)) #pertain action random\n",
        "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
        "\n",
        "            # Apply the sampled action in our environment\n",
        "            action = advanced_agent(state, env.configuration, action)\n",
        "            state = trainer.step(action)[0] # Step the environment with action\n",
        "            gain=state.players[0][0]/5000 # get reward from step and norm\n",
        "            rewards_history.append(gain)\n",
        "            episode_reward += gain\n",
        "\n",
        "            if env.done:\n",
        "                state = trainer.reset()\n",
        "        # Update running reward to check condition for solving\n",
        "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "        #running reward is the sum of previous episode, episode reward is the current episode reward\n",
        "        # average the two to get a smoothed reward signal\n",
        "\n",
        "        # Calculate expected value from future rewards (critic value)\n",
        "        # - At each timestep what was the total reward received after that timestep\n",
        "        # - Rewards in the future are discounted by multiplying them with gamma\n",
        "        # - These are the labels for our critic\n",
        "        returns = []\n",
        "        discounted_sum = 0\n",
        "        for r in rewards_history[::-1]:\n",
        "            discounted_sum = r + gamma * discounted_sum\n",
        "            returns.insert(0, discounted_sum)\n",
        "        # Normalize\n",
        "        returns = np.array(returns)\n",
        "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
        "        returns = returns.tolist()\n",
        "        # Calculating loss values to update our network\n",
        "        history = zip(action_probs_history, critic_value_history, returns) # true critic value, predict critic value\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "        for log_prob, value, ret in history:\n",
        "            # At this point in history, the critic estimated that we would get a\n",
        "            # total reward = `value` in the future. We took an action with log probability\n",
        "            # of `log_prob` and ended up recieving a total reward = `ret`.\n",
        "            # The actor must be updated so that it predicts an action that leads to\n",
        "            # high rewards (compared to critic's estimate) with high probability.\n",
        "            diff = ret - value\n",
        "            actor_losses.append(-log_prob * diff)  # actor loss #action with log probability*(true-predict critic value)\n",
        "\n",
        "            # The critic must be updated so that it predicts a better estimate of\n",
        "            # the future rewards.\n",
        "            critic_losses.append( #true-predict critic value\n",
        "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
        "            )\n",
        "        # Backpropagation\n",
        "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables) #calculate gradients\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables)) #update network weights\n",
        "\n",
        "        # Clear the loss and reward history\n",
        "        action_probs_history.clear()\n",
        "        critic_value_history.clear()\n",
        "        rewards_history.clear()\n",
        "\n",
        "    # Log details\n",
        "    episode_count += 1\n",
        "    if episode_count % 10 == 0:\n",
        "        template = \"running reward: {:.2f} at episode {}\"\n",
        "        print(template.format(running_reward, episode_count))\n",
        "\n",
        "    if running_reward > 550:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EfsKArk-Y-1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "while not env.done:\n",
        "    state_ = tf.convert_to_tensor(state.halite)\n",
        "    state_ = tf.expand_dims(state_, 0)\n",
        "    action_probs, critic_value = model(state_)\n",
        "    critic_value_history.append(critic_value[0, 0])\n",
        "    action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
        "    action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
        "    action = advanced_agent(state, env.configuration, action)\n",
        "    state = trainer.step(action)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1044otl7FqGi"
      },
      "source": [
        "## Results\n",
        "The Yellow ships and shipyards are controlled by our trained actor-critic model and the red ship and shipyards are trained against the random predicting agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5DD5b6v-Y-8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "env.render(mode=\"ipython\",width=800, height=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yqchxyk0FqGn",
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Designing game AI with Reinforcement learning",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
